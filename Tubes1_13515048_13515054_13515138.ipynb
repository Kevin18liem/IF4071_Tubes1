{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tubes 1\n",
    "### Menulis kode clustering untuk kmeans, kmedoids, agglomerative, dbscan\n",
    "___Alvin Sullivan 13515048___\n",
    "\n",
    "___Albertus Djauhari Djohan 13515054___\n",
    "\n",
    "___Kevin 13515138___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "Fungsi untuk membaca data dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    file = open(filename,'r')\n",
    "\n",
    "    data_file = [[float(val) for val in line.split()] for line in file if len(line.strip()) > 0]\n",
    "\n",
    "    file.close()\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix data structure\n",
    "Struktur data untuk menyimpan jarak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from enum import IntEnum\n",
    "\n",
    "class type_matrix(IntEnum):\n",
    "    \n",
    "    EUCLIDEAN = 0\n",
    "    MANHATTAN = 2\n",
    "\n",
    "class distance_matrix:\n",
    "    def __init__(self, matrix_type, **kwargs):\n",
    "        self.__type = matrix_type\n",
    "        self.__args = kwargs\n",
    "        self.__func = self.__args.get('func', None)\n",
    "        self.__calculator = self.__create_calculator_distance()\n",
    "\n",
    "    def __call__(self, object1, object2):\n",
    "        return self.__calculator(object1, object2)\n",
    "    def __create_calculator_distance(self):\n",
    "\n",
    "        if self.__type == type_matrix.EUCLIDEAN:\n",
    "                return euclidean_distance\n",
    "        elif self.__type == type_matrix.MANHATTAN:\n",
    "                return manhattan_distance_numpy\n",
    "    def get_distance_type(self):\n",
    "        return (self.__type)\n",
    "\n",
    "def euclidean_distance(object1, object2):\n",
    "    return numpy.sqrt(numpy.sum(numpy.square(object1 - object2), axis=1).T)\n",
    "def manhattan_distance_numpy(object1, object2):\n",
    "    return numpy.sum(numpy.absolute(object1 - object2), axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix with linkage data structure\n",
    "Struktur data untuk menyimpan jarak sesuai dengan tipe linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "class LinkageDistanceMatrix:\n",
    "    def __init__(self, data, clusters, linkage = \"single\", affinity = \"euclidean\"):\n",
    "        self.data = data\n",
    "        self.clusters = clusters\n",
    "        self.linkage = linkage\n",
    "        self.affinity = affinity\n",
    "        self.matrix = self.create_distance_matrix()\n",
    "    \n",
    "    def create_distance_matrix(self):\n",
    "        if (self.linkage == \"complete\"):\n",
    "            return self.create_complete_distance_matrix()\n",
    "        elif (self.linkage == \"average\"):\n",
    "            return self.create_average_distance_matrix()\n",
    "        elif (self.linkage == \"average_group\"):\n",
    "            return self.create_average_group_distance_matrix()\n",
    "        else:\n",
    "            return self.create_single_distance_matrix()\n",
    "    \n",
    "    def create_single_distance_matrix(self):\n",
    "        matrix = numpy.zeros((len(self.clusters), len(self.clusters)))\n",
    "        for matrix1 in range(len(self.clusters)):\n",
    "            for matrix2 in range(matrix1, len(self.clusters)):\n",
    "                distance = float('inf')\n",
    "                for index1 in self.clusters[matrix1]:\n",
    "                    for index2 in self.clusters[matrix2]:\n",
    "                        pairdistance = 0\n",
    "                        if (self.affinity == \"manhattan\"):\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + abs(self.data[index2, feature] - self.data[index1, feature])\n",
    "                        else:\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + (self.data[index2, feature] - self.data[index1, feature]) ** 2\n",
    "                            pairdistance = math.sqrt(pairdistance)\n",
    "                        distance = min(distance, pairdistance)\n",
    "                matrix[matrix1, matrix2] = distance\n",
    "                matrix[matrix2, matrix1] = distance\n",
    "        return matrix\n",
    "    \n",
    "    def create_complete_distance_matrix(self):\n",
    "        matrix = numpy.zeros((len(self.clusters), len(self.clusters)))\n",
    "        for matrix1 in range(len(self.clusters)):\n",
    "            for matrix2 in range(matrix1, len(self.clusters)):\n",
    "                distance = -1\n",
    "                for index1 in self.clusters[matrix1]:\n",
    "                    for index2 in self.clusters[matrix2]:\n",
    "                        pairdistance = 0\n",
    "                        if (self.affinity == \"manhattan\"):\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + abs(self.data[index2, feature] - self.data[index1, feature])\n",
    "                        else:\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + (self.data[index2, feature] - self.data[index1, feature]) ** 2\n",
    "                            pairdistance = math.sqrt(pairdistance)\n",
    "                        distance = max(distance, pairdistance)\n",
    "                matrix[matrix1, matrix2] = distance\n",
    "                matrix[matrix2, matrix1] = distance\n",
    "        return matrix\n",
    "\n",
    "    def create_average_distance_matrix(self):\n",
    "        matrix = numpy.zeros((len(self.clusters), len(self.clusters)))\n",
    "        for matrix1 in range(len(self.clusters)):\n",
    "            for matrix2 in range(matrix1, len(self.clusters)):\n",
    "                distance = 0\n",
    "                for index1 in self.clusters[matrix1]:\n",
    "                    for index2 in self.clusters[matrix2]:\n",
    "                        pairdistance = 0\n",
    "                        if (self.affinity == \"manhattan\"):\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + abs(self.data[index2, feature] - self.data[index1, feature])\n",
    "                        else:\n",
    "                            for feature in range(self.data[0].size):\n",
    "                                pairdistance = pairdistance + (self.data[index2, feature] - self.data[index1, feature]) ** 2\n",
    "                            pairdistance = math.sqrt(pairdistance)\n",
    "                        distance += pairdistance\n",
    "                distance /= (len(self.clusters[matrix1]) * len(self.clusters[matrix2]))\n",
    "                matrix[matrix1, matrix2] = distance\n",
    "                matrix[matrix2, matrix1] = distance\n",
    "        return matrix\n",
    "\n",
    "    def create_average_group_distance_matrix(self):\n",
    "        matrix = numpy.zeros((len(self.clusters), len(self.clusters)))\n",
    "        for matrix1 in range(len(self.clusters)):\n",
    "            for matrix2 in range(matrix1, len(self.clusters)):\n",
    "                center1 = numpy.zeros((1, self.data[0].size))\n",
    "                center2 = numpy.zeros((1, self.data[0].size))\n",
    "                for index1 in self.clusters[matrix1]:\n",
    "                    center1 += self.data[index1]\n",
    "                center1 /= len(self.clusters[matrix1])\n",
    "                for index2 in self.clusters[matrix2]:\n",
    "                    center2 += self.data[index2]\n",
    "                center2 /= len(self.clusters[matrix2])\n",
    "                distance = 0\n",
    "                if (self.affinity == \"manhattan\"):\n",
    "                    for feature in range(self.data[0].size):\n",
    "                        distance = distance + abs(center2[0, feature] - center1[0, feature])\n",
    "                else:\n",
    "                    for feature in range(self.data[0].size):\n",
    "                        distance = distance + (center2[0, feature] - center1[0, feature]) ** 2\n",
    "                    distance = math.sqrt(distance)\n",
    "                matrix[matrix1, matrix2] = distance\n",
    "                matrix[matrix2, matrix1] = distance\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "Model pembelajaran KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matrix\n",
    "from matrix import distance_matrix, type_matrix\n",
    "\n",
    "\n",
    "class kmeans:\n",
    "    def __init__(self,data,initial_centroids, tolerance, **kwargs):\n",
    "        self.__data = numpy.matrix(data)\n",
    "        self.__clusters = []\n",
    "        self.__centroids = numpy.matrix(initial_centroids)\n",
    "        self.__tolerance = tolerance\n",
    "        self.__matrix = kwargs.get('matrix', distance_matrix(type_matrix.EUCLIDEAN))\n",
    "        \n",
    "    def process(self):\n",
    "        if (len(self.__data[0])) != len(self.__centroids[0]):\n",
    "            raise NameError('Dimension of the input data and dimension of the initial cluster centers must be equal.')\n",
    "\n",
    "        maximum_change = float('inf')\n",
    "        stop_condition = self.__tolerance\n",
    "        while maximum_change > stop_condition:\n",
    "            self.__clusters = self.__update_clusters()\n",
    "            update_centroids = self.__update_centroids()\n",
    "            if len(self.__centroids) != len(update_centroids):\n",
    "                maximum_change = float('inf')\n",
    "            else:\n",
    "                changes = self.__matrix(self.__centroids, update_centroids)\n",
    "                maximum_change = numpy.max(changes)\n",
    "            self.__centroids = update_centroids.tolist()\n",
    "    def get_clusters(self):\n",
    "        return self.__clusters\n",
    "    def get_centroids(self):\n",
    "        if isinstance(self.__centroids, list):\n",
    "            return self.__centroids \n",
    "        return self.__centroids.tolist()\n",
    "    def __update_clusters(self):\n",
    "        clusters = [[] for _ in range(len(self.__centroids))]\n",
    "        dataset_diff = numpy.zeros((len(clusters), len(self.__data)))\n",
    "        for index_centroid in range(len(self.__centroids)):\n",
    "            dataset_diff[index_centroid] = self.__matrix(self.__data, self.__centroids[index_centroid])\n",
    "\n",
    "        optimum_indexes = numpy.argmin(dataset_diff, axis=0)\n",
    "        for index_point in range(len(optimum_indexes)):\n",
    "            index_cluster = optimum_indexes[index_point]\n",
    "            clusters[index_cluster].append(index_point)\n",
    "        clusters = [cluster for cluster in clusters if len(cluster)>0]\n",
    "        return clusters\n",
    "    def __update_centroids(self):\n",
    "        dimension = self.__data.shape[1]\n",
    "        centroids = numpy.zeros((len(self.__clusters),dimension))\n",
    "\n",
    "        for index in range(len(self.__clusters)):\n",
    "            cluster_points = self.__data[self.__clusters[index], :]\n",
    "            centroids[index] = cluster_points.mean(axis = 0)\n",
    "        return numpy.matrix(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMedoids\n",
    "Model pembelajaran KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matrix\n",
    "import random\n",
    "import numpy\n",
    "from matrix import distance_matrix, type_matrix\n",
    "\n",
    "class kmedoids:\n",
    "\n",
    "    def __init__(self, data, initial_medoids_index, tolerance, **kwargs):\n",
    "        self.__data = data\n",
    "        self.__clusters = []\n",
    "        self.index_medoids = initial_medoids_index\n",
    "        self.__tolerance = tolerance\n",
    "        self.__matrix = kwargs.get('matrix', distance_matrix(type_matrix.EUCLIDEAN))\n",
    "        # self.distance_calculator = self.__create_distance_calculator()\n",
    "        self.medoids_matrix_checker = numpy.zeros((len(initial_medoids_index), len(data)))\n",
    "        for i in range(len(initial_medoids_index)):\n",
    "            self.medoids_matrix_checker[i][initial_medoids_index[i]] = 1\n",
    "    def process(self):\n",
    "\n",
    "        diff = float('inf')\n",
    "        stop_conditon = self.__tolerance\n",
    "        counter = 0\n",
    "        while (True):\n",
    "            # Calculate Old Absolute Error\n",
    "            self.__clusters = self.__update_clusters()\n",
    "            old_absolute_error = self.calculate_absolute_error(self.__clusters, self.index_medoids)\n",
    "            # Calculate New Absolute Error\n",
    "            # temp_index_medoids = self.index_medoids\n",
    "            temp_index_medoids = self.__update_medoids()\n",
    "            new_temporary_clusters = self.__update_clusters()\n",
    "            new_absolute_error = self.calculate_absolute_error(new_temporary_clusters, temp_index_medoids)\n",
    "            # Calculate Difference\n",
    "            diff_error =  new_absolute_error - old_absolute_error\n",
    "            if diff_error < 0:\n",
    "                self.index_medoids = temp_index_medoids\n",
    "            else:\n",
    "                break\n",
    "    def get_clusters(self):\n",
    "\n",
    "        return self.__clusters\n",
    "\n",
    "    def get_medoids(self):\n",
    "        \n",
    "        return self.index_medoids\n",
    "\n",
    "    def __update_clusters(self):\n",
    "        clusters = [[self.index_medoids[i]] for i in range(len(self.index_medoids))]\n",
    "        for index_point in range(len(self.__data)):\n",
    "            if index_point in self.index_medoids:\n",
    "                continue\n",
    "            \n",
    "            index_optim = -1\n",
    "            dist_optim = float('Inf')\n",
    "\n",
    "            for index in range(len(self.index_medoids)):\n",
    "                dist = self.__matrix(numpy.matrix(self.__data[index_point]), numpy.matrix(self.__data[self.index_medoids[index]]))\n",
    "                if dist < dist_optim:\n",
    "                    index_optim = index\n",
    "                    dist_optim = dist\n",
    "            clusters[index_optim].append(index_point)\n",
    "        return clusters\n",
    "\n",
    "    def __update_medoids(self):\n",
    "        medoid_indexes = self.index_medoids\n",
    "        random_index_to_change = random.randint(0,len(self.index_medoids)-1)\n",
    "        random_value = random.randint(0,len(self.__data)-1)\n",
    "        while self.medoids_matrix_checker[random_index_to_change][random_value] == 1:\n",
    "            random_index_to_change = random.randint(0,len(self.index_medoids)-1)\n",
    "            random_value = random.randint(0,len(self.__data)-1)\n",
    "        medoid_indexes[random_index_to_change] = random_value\n",
    "        self.medoids_matrix_checker[random_index_to_change][random_value] = 1\n",
    "        return medoid_indexes\n",
    "    def calculate_absolute_error(self, clusters, index_medoids):\n",
    "        medoids = []\n",
    "        for i in index_medoids:\n",
    "            medoids.append(self.__data[i])\n",
    "        sum = 0\n",
    "        for i in range(len(index_medoids)):\n",
    "            for j in range(len(clusters[i])):\n",
    "                data = self.__data[clusters[i][j]]\n",
    "                sum += numpy.sum(numpy.absolute(numpy.array(data) - numpy.array(medoids[i])),axis=0)\n",
    "        return (sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "Model pembelajaran DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "class dbscan:\n",
    "\tdef __init__(self, data, epsilon, min_pts, distance_type=1):\n",
    "\t\tself.data = data\n",
    "\t\tself.clusters = []\n",
    "\t\tself.clusters_id = []\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.min_pts = min_pts\n",
    "\t\tself.neighborhood_list = [[] for _ in self.data]\n",
    "\t\tself.distance_type = distance_type\n",
    "\t\tself.core_points = []\n",
    "\t\tself.outlier = []\n",
    "\n",
    "\tdef compare(self, cluster1, cluster2):\n",
    "\t\tfirst_element = cluster1[0]\n",
    "\n",
    "\t\tif first_element in cluster2:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef process(self):\n",
    "\t\tfor i in range(0, len(self.data)):\n",
    "\t\t\tfor j in range(0, len(self.data)):\n",
    "\t\t\t\tif i!=j and self.calculate_distance(self.data[i], self.data[j]) <= self.epsilon:\n",
    "\t\t\t\t\tself.neighborhood_list[i].append(j)\n",
    "\n",
    "\t\tfor i in range(0, len(self.neighborhood_list)):\n",
    "\t\t\tif len(self.neighborhood_list[i]) + 1 >= self.min_pts:\n",
    "\t\t\t\tself.core_points.append(i)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.outlier.append(i)\n",
    "\n",
    "\t\tfor i in range(0, len(self.core_points)):\n",
    "\t\t\tfound = False\n",
    "\t\t\tcurrent_cluster = self.neighborhood_list[i]\n",
    "\t\t\tcurrent_cluster.append(i)\n",
    "\n",
    "\t\t\tfor cluster in self.clusters:\n",
    "\t\t\t\tif self.compare(cluster, current_cluster):\n",
    "\t\t\t\t\tfound = True\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif not found:\n",
    "\t\t\t\tself.clusters.append(current_cluster)\n",
    "\n",
    "\tdef calculate_distance(self, instance1, instance2):\n",
    "\t\tEUCLIDEAN = 1\n",
    "\t\tEUCLIDEAN_SQUARE = 2\n",
    "\t\tMANHATTAN = 3\n",
    "\n",
    "\t\tdiff = 0\n",
    "\n",
    "\t\tif self.distance_type == EUCLIDEAN :\n",
    "\t\t\tfor val1, val2 in zip(instance1, instance2):\n",
    "\t\t\t\tdiff += (val1 - val2)**2\n",
    "\t\t\tdiff = math.sqrt(diff)\n",
    "\t\telif self.distance_type == EUCLIDEAN_SQUARE :\n",
    "\t\t\tfor val1, val2 in zip(instance1, instance2):\n",
    "\t\t\t\tdiff += (val1 - val2)**2\n",
    "\t\telse:\n",
    "\t\t\tfor val1, val2 in zip(instance1, instance2):\n",
    "\t\t\t\tdiff += math.fabs(val1 - val2)\n",
    "\t\t\n",
    "\t\treturn diff\n",
    "\n",
    "\tdef get_clusters(self):\n",
    "\t\treturn self.clusters\n",
    "\n",
    "\tdef get_outliers(self):\n",
    "\t\treturn self.outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative\n",
    "\n",
    "Agglomerative merupakan hierarchical clustering untuk mengelompokkan data dari 1 instance dalam 1 cluster diiterasi hingga seluruh instance dalam 1 cluster. Algoritma menyatukan 2 cluster di setiap iterasinya dengan jarak terpendek antar cluster tersebut. Jarak antar cluster dapat dihitung dengan 4 linkage, yaitu single linkage, complete linkage, average linkage, atau average group linkage. Jarak antar titik dapat dihitung dengan 2 cara, yaitu euclidean atau manhattan.\n",
    "\n",
    "### Pseudo-code Agglomerative\n",
    "\n",
    "```python\n",
    "def fit(data):\n",
    "    initialize_clusters\n",
    "    while current_n_clusters > desired_n_clusters:\n",
    "    for each cluster in current_clusters:\n",
    "        for each cluster in current_clusters:\n",
    "            calculate_cluster_distance_to_distance_matrix\n",
    "    initialize_index\n",
    "    initialize_minimum_distance\n",
    "    for each cluster in distance_matrix:\n",
    "        for each cluster in distance_matrix:\n",
    "            if distance_of_cluster_pair < minimum_distance:\n",
    "                replace_index_with_cluster_pair\n",
    "                replace_minimum_distance_with_distance\n",
    "    initialize_new_clusters\n",
    "    for cluster in current_clusters:\n",
    "        if current_cluster = first_minimum_cluster:\n",
    "            append_new_clusters_with_appended_minimum_cluster_pair\n",
    "        else:\n",
    "            append_new_clusters_with_current_cluster\n",
    "    return new_clusters\n",
    "```\n",
    "\n",
    "### Penjelasan Kode Implementasi Agglomerative\n",
    "\n",
    "Agglomerative diimplementasi dengan sebuah kelas Agglomerative yang dapat menerima parameter n_clusters, linkage, dan affinity, di mana n_clusters menyatakan banyak cluster yang akan terbentuk, linkage menyatakan jenis linkage antar cluster yang digunakan pada perhitungan distance matrix, dan affinity menyatakan cara perhitungan jarak antar titik. Kelas memanggil fit untuk melakukan pembelajaran terhadap data masukan sesuai dengan penjelasan algoritma sebelumnya. Setiap iterasi menghasilkan daftar setiap cluster yang terbentuk dan berisi indeks instance yang berada pada setiap cluster beserta dengan distance matrix-nya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from linkagematrix import LinkageDistanceMatrix\n",
    "\n",
    "class Agglomerative:\n",
    "    def __init__(self, n_clusters = 2, linkage = \"single\", affinity = \"euclidean\"):\n",
    "        self.data = numpy.array([])\n",
    "        self.clusters = []\n",
    "        self.n_clusters = n_clusters\n",
    "        self.linkage = linkage\n",
    "        self.affinity = affinity\n",
    "        self.distance_matrix = numpy.array([])\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.data = numpy.matrix(data)\n",
    "        for index in range(len(data)):\n",
    "            self.clusters.append([index])\n",
    "        while (len(self.clusters) > self.n_clusters):\n",
    "            self.distance_matrix = LinkageDistanceMatrix(self.data, self.clusters, self.linkage, self.affinity)\n",
    "            self.clusters = self.update_clusters()\n",
    "\n",
    "    def get_clusters(self):\n",
    "        return self.clusters\n",
    "\n",
    "    def update_clusters(self):\n",
    "        clusters = []\n",
    "        minindexi = 0\n",
    "        minindexj = 0\n",
    "        mindistance = float('inf')\n",
    "        for indexi in range(len(self.clusters)):\n",
    "            for indexj in range(indexi, len(self.clusters)):\n",
    "                if (indexi != indexj and self.distance_matrix.matrix[indexi, indexj] < mindistance):\n",
    "                    minindexi = indexi\n",
    "                    minindexj = indexj\n",
    "                    mindistance = self.distance_matrix.matrix[indexi, indexj]\n",
    "        iterator = -1\n",
    "        for cluster in self.clusters:\n",
    "            iterator += 1\n",
    "            if (iterator == minindexi):\n",
    "                clusters.append(self.clusters[minindexi] + self.clusters[minindexj])\n",
    "            elif (iterator != minindexj):\n",
    "                clusters.append(self.clusters[iterator])\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi\n",
    "Evaluasi model pembelajaran yang diimplementasi dibandingkan dengan label data iris sesungguhnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Fungsi untuk evaluasi___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]]\n"
     ]
    }
   ],
   "source": [
    "true_clusters = [[i for i in range(0, 50)], [i for i in range(50, 100)], [i for i in range(100, 150)]]\n",
    "print(true_clusters)\n",
    "\n",
    "def evaluate(prediction_clusters, true_clusters):\n",
    "    accuracy = []\n",
    "    \n",
    "    for prediction_cluster in prediction_clusters:\n",
    "        max_true = 0\n",
    "        \n",
    "        for true_cluster in true_clusters:\n",
    "            counter = 0\n",
    "            \n",
    "            for element in prediction_cluster:\n",
    "                if element in true_cluster:\n",
    "                    counter+=1\n",
    "                    \n",
    "            if max_true < counter:\n",
    "                max_true = counter\n",
    "            \n",
    "        accuracy.append(max_true/len(prediction_cluster) * 100)\n",
    "        \n",
    "    return sum(accuracy)/len(accuracy)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(filename, tolerance, start_centroids):\n",
    "    sample = read_data(filename)\n",
    "    kmeans_instance = kmeans(sample, start_centroids, tolerance=0.25)\n",
    "    kmeans_instance.process()\n",
    "    clusters = kmeans_instance.get_clusters()\n",
    "    centroids = kmeans_instance.get_centroids()\n",
    "    print (\"Cluster Result: \\n\", clusters)\n",
    "    print (\"Centroids Result: \\n\",centroids)\n",
    "    print (\"Akurasi: \", evaluate(clusters, true_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmedoids_clustering(filename, tolerance, start_medoids):\n",
    "    sample = read_data(filename)\n",
    "    kmedoids_instance = kmedoids(sample,start_medoids,tolerance=0)\n",
    "    kmedoids_instance.process()\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "    medoids = kmedoids_instance.get_medoids()\n",
    "    print (\"Cluster Result: \\n\", clusters)\n",
    "    print (\"Centroids Result: \\n\",medoids)\n",
    "    print (\"Akurasi: \", evaluate(clusters, true_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(filename, epsilon, min_pts):\n",
    "    sample = read_data(filename)\n",
    "    dbscan_instance = dbscan(sample, epsilon, min_pts)\n",
    "    dbscan_instance.process()\n",
    "    clusters = dbscan_instance.get_clusters()\n",
    "    outliers = dbscan_instance.get_outliers()\n",
    "    print(\"Clusters :\\n\", clusters)\n",
    "    print(\"Outliers :\\n\", outliers)\n",
    "    print(\"Akurasi: \", evaluate(clusters, true_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_clustering(filename, n_clusters, linkage, affinity):\n",
    "    sample = read_data(filename)\n",
    "    agglomerative_instance = Agglomerative(n_clusters, linkage, affinity)\n",
    "    agglomerative_instance.fit(sample)\n",
    "    clusters = agglomerative_instance.get_clusters()\n",
    "    print(linkage, affinity)\n",
    "    print(\"Cluster Result: \\n\", clusters)\n",
    "    print (\"Akurasi: \", evaluate(clusters, true_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_centroids = [[5.1,3.5,1.4,0.2],[4.9,3.0,1.4,0.2],[4.7,3.2,1.3,0.2]]\n",
    "start_medoids = [0,2,7]\n",
    "filename = \"./dataset/iris_without_label.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi KMeans\n",
    "\n",
    "KMeans yang diimplementasi terhadap dataset iris menghasilkan 3 cluster dengan centroidnya masing-masing. Akurasi yang dihasilkan jika dibandingkan dengan label dataset iris sesungguhnya yaitu 90.03%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Result: \n",
      " [[50, 51, 52, 54, 56, 58, 63, 65, 70, 72, 73, 75, 76, 77, 83, 85, 86, 91, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [53, 55, 57, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 74, 78, 79, 80, 81, 82, 84, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 106], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]]\n",
      "Centroids Result: \n",
      " [[6.570149253731341, 2.988059701492538, 5.338805970149254, 1.885074626865671], [5.636363636363636, 2.6363636363636362, 4.027272727272727, 1.2515151515151515], [5.005999999999999, 3.4180000000000006, 1.464, 0.2439999999999999]]\n",
      "Akurasi:  90.03467510930197\n"
     ]
    }
   ],
   "source": [
    "kmeans_clustering(filename, 1e-3, start_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi KMedoids\n",
    "\n",
    "KMedoids yang diimplementasi terhadap dataset iris menghasilkan 3 cluster dengan centroidnya masing-masing. Akurasi yang dihasilkan jika dibandingkan dengan label dataset iris sesungguhnya yaitu 82.24%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Result: \n",
      " [[6, 1, 2, 3, 8, 11, 12, 13, 22, 29, 30, 38, 41, 42, 45, 47], [0, 4, 5, 7, 9, 10, 14, 15, 16, 17, 18, 19, 20, 21, 27, 28, 32, 33, 34, 35, 36, 37, 39, 40, 46, 48, 49], [23, 24, 25, 26, 31, 43, 44, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]]\n",
      "Centroids Result: \n",
      " [14, 0, 23]\n",
      "Akurasi:  82.2429906542056\n"
     ]
    }
   ],
   "source": [
    "kmedoids_clustering(filename,0,start_medoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi DBSCAN\n",
    "\n",
    "DBSCAN yang diimplementasi terhadap dataset iris menghasilkan 3 cluster tanpa ada outlier. Akurasi yang dihasilkan jika dibandingkan dengan label dataset iris sesungguhnya yaitu 75.07%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters :\n",
      " [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 57, 64, 79, 93, 98, 0], [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 50], [50, 52, 56, 58, 76, 77, 83, 86, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 114, 115, 116, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 117]]\n",
      "Outliers :\n",
      " []\n",
      "Akurasi:  75.07427213309566\n"
     ]
    }
   ],
   "source": [
    "dbscan_clustering(filename, 2.7, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi Agglomerative\n",
    "\n",
    "Agglomerative yang diimplementasi terhadap dataset iris menghasilkan 3 cluster. Akurasi yang dihasilkan jika dibandingkan dengan label dataset iris sesungguhnya yaitu 83.67%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single euclidean\n",
      "Cluster Result: \n",
      " [[0, 17, 40, 4, 7, 39, 49, 27, 28, 35, 10, 48, 23, 26, 43, 1, 45, 12, 9, 34, 37, 29, 30, 2, 3, 47, 25, 8, 38, 42, 11, 6, 19, 21, 46, 13, 24, 36, 20, 31, 5, 18, 16, 32, 33, 44, 15, 14, 22, 41], [50, 52, 86, 51, 56, 54, 58, 65, 75, 74, 97, 77, 76, 71, 53, 89, 69, 80, 81, 67, 82, 92, 88, 94, 95, 96, 99, 90, 61, 55, 66, 84, 63, 91, 78, 73, 79, 85, 59, 70, 127, 138, 123, 126, 146, 149, 101, 142, 113, 121, 72, 83, 133, 103, 116, 137, 104, 128, 132, 110, 147, 111, 141, 145, 112, 139, 120, 143, 140, 144, 124, 115, 136, 148, 102, 125, 129, 64, 100, 119, 107, 130, 114, 62, 68, 87, 105, 122, 118, 135, 134, 108, 109, 57, 93, 60, 98, 106], [117, 131]]\n",
      "Akurasi:  83.6734693877551\n"
     ]
    }
   ],
   "source": [
    "agglomerative_clustering(filename, 3, \"single\", \"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Tugas\n",
    "1. Alvin Sullivan - 13515048 - Agglomerative\n",
    "2. Albertus Djauhari - 13515054 - DBSCAN\n",
    "3. Kevin - 13515138 - KMeans & KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
