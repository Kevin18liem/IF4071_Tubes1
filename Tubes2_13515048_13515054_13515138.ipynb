{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tubes 2\n",
    "## Feed Forward Neural Network\n",
    "___Alvin Sullivan 13515048___\n",
    "\n",
    "___Albertus Djauhari Djohan 13515054___\n",
    "\n",
    "___Kevin 13515138___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Algoritma Backpropagation\n",
    "\n",
    "Classifier dibuat dengan sebuah kelas bernama `MultiLayerNN`. Kelas ini berfungsi untuk memodelkan neural network yang mampu melakukan pembelajaran dengan mini-batch stochastic gradient descent. Kelas ini memiliki atribut matriks data input tanpa label, matriks weight dari hidden node, matriks weight dari output node, banyak batch, konstanta learning rate, konstanta tolerance, konstanta momentum, dan banyak epochs. Kelas ini memiliki spesifikasi sebagai berikut.\n",
    "\n",
    "- Jumlah hidden layer maksimal 10\n",
    "- Jumlah node dalam setiap hidden layer dapat bervariasi\n",
    "- Fully-connected layer\n",
    "- Fungsi aktivasi berupa sigmoid untuk semua hidden layer maupun output layer\n",
    "- Node output berjumlah 1\n",
    "- Program memberikan pilihan untuk menggunakan momentum atau tidak\n",
    "- Program mengimplementasikan mini-batch stochastic gradient descent\n",
    "\n",
    "Kelas ini memiliki fungsi `train` untuk melakukan pembelajaran mini-batch stochastic gradient descent. Fungsi train melakukan pembelajaran sesuai dengan epochs dan batch yang ditentukan. Untuk setiap batch dalam epochs, dipanggil fungsi `gradient_descent` yang memanggil tiga fungsi lainnya secara berurutan sesuai algoritma gradient descent. Pertama dipanggil fungsi `feed_forward` untuk menentukan output setiap neuron. Kedua dipanggil fungsi `back_propagation` untuk menentukan delta setiap neuron. Ketiga dipanggil fungsi `update_weight` untuk mengubah weight setiap neuron sesuai dengan hasil dari fungsi-fungsi sebelumnya. Setelah seluruh epochs selesai, maka diperoleh sebuah model neural network dengan representasi matriks weight setiap neuron yang sudah diperbarui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_layer_num = 4\n",
    "var_nodes = [4, 3 , 2, 3 ,1]\n",
    "var_epoch = 50\n",
    "var_momentum = 0.001\n",
    "var_learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNN:\n",
    "\n",
    "    def __init__(self, data, hidden_node, output_node,\n",
    "        num_batch, learning_rate_const, tolerance_const,\n",
    "        **kwargs):\n",
    "        \n",
    "        self.instance = data\n",
    "        self.w_hidden_node = hidden_node\n",
    "        self.w_output_node = output_node\n",
    "        # Gradient Descent Parameters\n",
    "        self.batch_size = num_batch\n",
    "        self.learning_rate = learning_rate_const\n",
    "        self.tolerance = tolerance_const\n",
    "        self.momentum = kwargs.get('momentum', 0)\n",
    "        self.epochs = kwargs.get('epochs', 10)\n",
    "       \n",
    "    def train(self, instance_target):\n",
    "        instance_target_t = np.array([instance_target]).T\n",
    "        batch_iteration = int(np.ceil(self.instance.shape[0] / self.batch_size))\n",
    "        old_loss = -np.inf\n",
    "        for step in range(self.epochs):\n",
    "            loss = 0\n",
    "            for i in range(batch_iteration):\n",
    "                start_index = i * self.batch_size\n",
    "                end_index = i * self.batch_size + self.batch_size\n",
    "                if end_index > len(instance_target_t):\n",
    "                    end_index = len(instance_target_t)\n",
    "                o_out = self.gradient_descent(self.instance[start_index:end_index:1], instance_target_t[start_index:end_index:1])\n",
    "                loss = loss + self.loss_function(o_out, instance_target_t[start_index:end_index:1])\n",
    "        # Print Loss\n",
    "            print (\"Loss after epoch %i: %f\" % (step, loss/self.instance.shape[0]))\n",
    "            \n",
    "            if np.abs(loss - old_loss) < self.tolerance:\n",
    "                break\n",
    "            old_loss = loss\n",
    "    \n",
    "    def feed_forward(self, instance):\n",
    "        s = list()\n",
    "        o = list()\n",
    "        # Feed Forward Hidden Node\n",
    "        s_temp = instance.dot(self.w_hidden_node[0].T)\n",
    "        o_temp = self.sigmoid(s_temp)\n",
    "        s.append(s_temp)\n",
    "        o.append(o_temp)\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(1,iteration):\n",
    "            s_temp = o[i-1].dot(self.w_hidden_node[i].T)\n",
    "            o_temp = self.sigmoid(s_temp)\n",
    "            o.append(o_temp)\n",
    "        # Feed Forward Output Node\n",
    "        s_out = o[-1].dot(self.w_output_node.T)\n",
    "        o_out = self.sigmoid(s_out)\n",
    "        return s, o, s_out, o_out\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        output = 1 / (1 + np.exp(-X))\n",
    "        return np.matrix(output)\n",
    "\n",
    "    def loss_function(self, o_out, instance_target):\n",
    "        \n",
    "        squared_error = np.square(instance_target - o_out)\n",
    "        data_loss = np.sum(squared_error)      \n",
    "        return data_loss    \n",
    "    def back_propagation(self, instance_target, o, o_out):\n",
    "        d = list()\n",
    "        # Back Propagation Output Node\n",
    "        d_temp = np.multiply(np.multiply(o_out, 1-o_out), instance_target-o_out)\n",
    "        d.insert(0, d_temp)\n",
    "        # Back Propagation Hidden Node\n",
    "        d_temp = np.multiply(np.multiply(o[-1], 1-o[-1]), (self.w_output_node.T.dot(d[0].T)).T)\n",
    "        d.insert(0, d_temp)\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(iteration-1, 0, -1):\n",
    "            d_temp = np.multiply(np.multiply(o[i-1], 1-o[i-1]), (self.w_hidden_node[i].T.dot(d[0].T)).T)\n",
    "            d.insert(0, d_temp)\n",
    "        return d\n",
    "\n",
    "    def update_weight(self, instance, o, d):\n",
    "        # Update Weight Output Node\n",
    "        self.w_output_node[0] = self.w_output_node[0] + self.momentum * self.w_output_node[0] + self.learning_rate * d[-1].T.dot(o[-1])\n",
    "        # Update Weight Hidden Node\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(iteration-1, 0, -1):\n",
    "            self.w_hidden_node[i] = self.w_hidden_node[i] + self.momentum * self.w_hidden_node[i] + self.learning_rate * d[i].T.dot(o[i-1])\n",
    "        self.w_hidden_node[0] = self.w_hidden_node[0] + self.momentum * self.w_hidden_node[0] + self.learning_rate * d[0].T.dot(instance)\n",
    "    def gradient_descent(self, instance, instance_target):\n",
    "        # Feed Forward\n",
    "        _,o,_,o_out = self.feed_forward(instance)\n",
    "        # Back Propagation      \n",
    "        d = self.back_propagation(instance_target, o, o_out)\n",
    "        # Update Weight\n",
    "        self.update_weight(instance, o, d)\n",
    "        return o_out\n",
    "    def predict(self, instance):\n",
    "        _,_,s_out,o_out = self.feed_forward(instance)\n",
    "        return o_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Keras\n",
    "\n",
    "Pada hasil implementasi keras dalam klasifikasi data weather, pertama telah dibuat suatu instans Sequential(). Pada model ini, akan diinisiasi arsitektur dari neural network yang akan dibangun. Dari input node, hidden layer besert jumlah hidden node dari masing-masing layaer, dan output node. Pada inisiasi ini juga ditentukan fungsi aktivasi yang digunakan.\n",
    "\n",
    "Setelah itu, eksplorasi ini juga mencoba menggunakan stochastic gradient descent optimizer untuk memasukkan fatkro learning rate, momentum dan decay factor pada neural net yang dibangun.\n",
    "\n",
    "Setelah itu, model akan menggunakan perhitungan loss mean_squared_error dan akurasi sebagai matriks pengukuran perfomansi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for i in range(0,len(var_nodes)):\n",
    "    if i == 0:\n",
    "        model.add(Dense(units=var_nodes[1], activation='sigmoid', input_dim=var_nodes[0]))\n",
    "        model.add(Dropout(0.1))\n",
    "    elif i == len(var_nodes)-1:\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(units=var_nodes[i+1], activation='sigmoid'))\n",
    "        model.add(Dropout(0.1))\n",
    "        \n",
    "sgd = optimizers.SGD(lr=0.01, decay=var_learning_rate, momentum=var_momentum, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan Hasil Algoritma Backpropagation dan Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksekusi Data Weather\n",
    "\n",
    "- Membaca dataset weather\n",
    "- Praproses Data (Continuous dan Kategorikal)\n",
    "    - Kategorikal menggunakan StandardScaler\n",
    "    - Continuous menggunakan LabelEncoder\n",
    "- Melakukan split training dengan skema hold-out 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    outlook      temp  humidity  windy  play\n",
      "0         2  1.804715  0.338726      0     0\n",
      "1         2  1.015152  0.843212      1     0\n",
      "2         0  1.488890  0.439623      0     1\n",
      "3         1 -0.563974  1.448595      0     1\n",
      "4         1 -0.879799 -0.165760      0     1\n",
      "5         1 -1.353537 -1.174731      1     0\n",
      "6         0 -1.511449 -1.679217      1     1\n",
      "7         2 -0.248148  1.347697      0     0\n",
      "8         2 -0.721886 -1.174731      0     1\n",
      "9         1  0.225589 -0.165760      0     1\n",
      "10        2  0.225589 -1.174731      1     1\n",
      "11        0 -0.248148  0.843212      1     1\n",
      "12        0  1.173065 -0.670245      0     1\n",
      "13        1 -0.406061  0.944109      1     0\n",
      "X_train\n",
      "[[ 2.          0.22558942 -1.17473092  1.        ]\n",
      " [ 1.         -0.40606095  0.9441089   1.        ]\n",
      " [ 0.          1.48889015  0.43962323  0.        ]\n",
      " [ 0.         -1.5114491  -1.67921659  1.        ]\n",
      " [ 2.         -0.72188614 -1.17473092  0.        ]\n",
      " [ 0.         -0.24814836  0.84321176  1.        ]\n",
      " [ 1.         -0.56397354  1.44859457  0.        ]\n",
      " [ 0.          1.17306497 -0.67024525  0.        ]\n",
      " [ 1.         -1.3535365  -1.17473092  1.        ]\n",
      " [ 2.          1.01515238  0.84321176  1.        ]\n",
      " [ 2.          1.80471534  0.33872609  0.        ]\n",
      " [ 1.         -0.87979873 -0.16575958  0.        ]]\n",
      "X_test\n",
      "[[ 1.          0.22558942 -0.16575958  0.        ]\n",
      " [ 2.         -0.24814836  1.34769743  0.        ]]\n",
      "y_train\n",
      "[1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "y_test\n",
      "[1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "file = \"dataset/weather.csv\"\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "#Handle Continuous Data\n",
    "scaled_features = data.copy()\n",
    "col_names = ['temp', 'humidity']\n",
    "scaled_features[col_names] = StandardScaler().fit_transform(\n",
    "                                        scaled_features[col_names])\n",
    "data[col_names] = scaled_features[col_names]\n",
    "\n",
    "#Handle Categorical Data\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['outlook'] = le.fit_transform(data['outlook'])\n",
    "data['windy'] = le.fit_transform(data['windy'])\n",
    "data['play'] = le.fit_transform(data['play'])\n",
    "print(data)\n",
    "data = data.values\n",
    "X = data[:, 0:-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=43)\n",
    "\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "\n",
    "print('X_test')\n",
    "print(X_test)\n",
    "\n",
    "print('y_train')\n",
    "print(y_train)\n",
    "\n",
    "print('y_test')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inisialisasi bobot awal hidden node dan output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks weight hidden node:\n",
      " [array([[ 0.88202617,  0.2000786 ,  0.48936899,  1.1204466 ],\n",
      "       [ 0.933779  , -0.48863894,  0.47504421, -0.0756786 ],\n",
      "       [-0.05160943,  0.20529925,  0.07202179,  0.72713675],\n",
      "       [ 0.38051886,  0.06083751,  0.22193162,  0.16683716]]), array([[ 0.86260696, -0.11844818,  0.18074972, -0.4931124 ],\n",
      "       [-1.47396936,  0.37736687,  0.49908247, -0.42848917],\n",
      "       [ 1.31044344, -0.83967841,  0.02641869, -0.10807065]]), array([[ 1.08383858,  1.03899355,  0.10956438],\n",
      "       [ 0.26740128, -0.62775932, -1.40063461]]), array([[-0.20086717,  0.09026812],\n",
      "       [ 0.71030866,  0.69419433],\n",
      "       [-0.22362324, -0.17453457]])]\n",
      "Matriks weight output node:\n",
      " [[-1.04855297 -1.42001794 -1.70627019]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "w_hidden_node = list()\n",
    "for i in range(0,len(var_nodes)-1):\n",
    "    if i == 0:\n",
    "        w_temp = np.random.randn(4, 4) / np.sqrt(4)\n",
    "    else:\n",
    "        w_temp = np.random.randn(var_nodes[i], var_nodes[i-1]) / np.sqrt(var_nodes[i])\n",
    "    w_hidden_node.append(w_temp)\n",
    "\n",
    "w_output_node = np.random.randn(var_nodes[-1], var_nodes[-2])\n",
    "\n",
    "print('Matriks weight hidden node:\\n', w_hidden_node)\n",
    "print('Matriks weight output node:\\n', w_output_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Mini-Batch (Batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_batch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.530392\n",
      "Loss after epoch 1: 0.490322\n",
      "Loss after epoch 2: 0.440204\n",
      "Loss after epoch 3: 0.385529\n",
      "Loss after epoch 4: 0.335448\n",
      "Loss after epoch 5: 0.296674\n",
      "Loss after epoch 6: 0.270165\n",
      "Loss after epoch 7: 0.253310\n",
      "Loss after epoch 8: 0.242938\n",
      "Loss after epoch 9: 0.236610\n",
      "Loss after epoch 10: 0.232736\n",
      "Loss after epoch 11: 0.230341\n",
      "Loss after epoch 12: 0.228844\n",
      "Loss after epoch 13: 0.227898\n",
      "Loss after epoch 14: 0.227294\n",
      "Loss after epoch 15: 0.226904\n",
      "Loss after epoch 16: 0.226651\n",
      "Loss after epoch 17: 0.226485\n",
      "Loss after epoch 18: 0.226375\n",
      "Loss after epoch 19: 0.226301\n",
      "Loss after epoch 20: 0.226251\n",
      "Loss after epoch 21: 0.226216\n",
      "Loss after epoch 22: 0.226191\n",
      "Loss after epoch 23: 0.226173\n",
      "Loss after epoch 24: 0.226159\n",
      "Loss after epoch 25: 0.226148\n",
      "Loss after epoch 26: 0.226139\n",
      "Loss after epoch 27: 0.226130\n",
      "Loss after epoch 28: 0.226123\n",
      "Loss after epoch 29: 0.226115\n",
      "Loss after epoch 30: 0.226108\n",
      "Loss after epoch 31: 0.226101\n",
      "Loss after epoch 32: 0.226094\n",
      "Loss after epoch 33: 0.226087\n",
      "Loss after epoch 34: 0.226079\n",
      "Loss after epoch 35: 0.226072\n",
      "Loss after epoch 36: 0.226065\n",
      "Loss after epoch 37: 0.226057\n",
      "Loss after epoch 38: 0.226049\n",
      "Loss after epoch 39: 0.226042\n",
      "Loss after epoch 40: 0.226034\n",
      "Loss after epoch 41: 0.226026\n",
      "Loss after epoch 42: 0.226018\n",
      "Loss after epoch 43: 0.226010\n",
      "Loss after epoch 44: 0.226002\n",
      "Loss after epoch 45: 0.225994\n",
      "Loss after epoch 46: 0.225986\n",
      "Loss after epoch 47: 0.225978\n",
      "Loss after epoch 48: 0.225970\n",
      "Loss after epoch 49: 0.225962\n",
      "Matriks weight hidden node:\n",
      " [matrix([[ 1.59615762,  0.36508357,  0.90670376,  2.03955252],\n",
      "        [ 1.67215243, -0.9008717 ,  0.84551071, -0.14119376],\n",
      "        [-0.10226558,  0.36606254,  0.12098327,  1.32017507],\n",
      "        [ 0.70342126,  0.11749435,  0.42028728,  0.31134833]]), matrix([[ 1.54130174, -0.2483885 ,  0.3161589 , -0.91631244],\n",
      "        [-2.67052829,  0.69890563,  0.92302761, -0.76740645],\n",
      "        [ 2.44754809, -1.47159889,  0.0864015 , -0.15487261]]), matrix([[ 2.0192962 ,  1.96180651,  0.24634719],\n",
      "        [ 0.45562945, -1.10426477, -2.58066935]]), matrix([[-0.84788704, -0.01488573],\n",
      "        [ 0.76385063,  1.07003926],\n",
      "        [-1.59628886, -0.77255791]])]\n",
      "Matriks weight output node:\n",
      " [[ 0.54128047  1.05132938 -0.98443072]]\n",
      "Hasil Prediksi Train Test: \n",
      "[[0.67410236]\n",
      " [0.67343017]]\n"
     ]
    }
   ],
   "source": [
    "multiLayerNN = MultiLayerNN(X_train, w_hidden_node, w_output_node, var_batch, var_learning_rate, 1e-6, momentum = var_momentum, epochs = var_epoch)\n",
    "multiLayerNN.train(y_train)\n",
    "print('Matriks weight hidden node:\\n', multiLayerNN.w_hidden_node)\n",
    "print('Matriks weight output node:\\n', multiLayerNN.w_output_node)\n",
    "\n",
    "y_test_res = multiLayerNN.predict(X_test)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(y_test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier menggunakan keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.2409 - acc: 0.6667\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2325 - acc: 0.6667\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2318 - acc: 0.6667\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2285 - acc: 0.6667\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2402 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2358 - acc: 0.6667\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2407 - acc: 0.6667\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2350 - acc: 0.6667\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2383 - acc: 0.6667\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2332 - acc: 0.6667\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2387 - acc: 0.6667\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2381 - acc: 0.6667\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2314 - acc: 0.6667\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2302 - acc: 0.6667\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2296 - acc: 0.6667\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2366 - acc: 0.6667\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2284 - acc: 0.6667\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2339 - acc: 0.6667\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2369 - acc: 0.6667\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2415 - acc: 0.6667\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2305 - acc: 0.6667\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2405 - acc: 0.6667\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2314 - acc: 0.6667\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2481 - acc: 0.6667\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2300 - acc: 0.6667\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2442 - acc: 0.6667\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2404 - acc: 0.6667\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2492 - acc: 0.6667\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2318 - acc: 0.6667\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2304 - acc: 0.6667\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2274 - acc: 0.6667\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2353 - acc: 0.6667\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2317 - acc: 0.6667\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2369 - acc: 0.6667\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2358 - acc: 0.6667\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2315 - acc: 0.6667\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2417 - acc: 0.6667\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2401 - acc: 0.6667\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2426 - acc: 0.6667\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2342 - acc: 0.6667\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2333 - acc: 0.6667\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2282 - acc: 0.6667\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2344 - acc: 0.6667\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2419 - acc: 0.6667\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2330 - acc: 0.6667\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2294 - acc: 0.6667\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2368 - acc: 0.6667\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2326 - acc: 0.6667\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2215 - acc: 0.6667\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2414 - acc: 0.6667\n",
      "[0.2531016767024994, 0.5]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=var_batch, epochs=var_epoch, verbose=1)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Mini-Batch (Batch_size = jumlah_data)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_batch = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.222158\n",
      "Loss after epoch 1: 0.222136\n",
      "Loss after epoch 2: 0.222120\n",
      "Loss after epoch 3: 0.222109\n",
      "Loss after epoch 4: 0.222100\n",
      "Loss after epoch 5: 0.222092\n",
      "Loss after epoch 6: 0.222086\n",
      "Loss after epoch 7: 0.222080\n",
      "Loss after epoch 8: 0.222075\n",
      "Loss after epoch 9: 0.222070\n",
      "Loss after epoch 10: 0.222066\n",
      "Loss after epoch 11: 0.222061\n",
      "Loss after epoch 12: 0.222057\n",
      "Loss after epoch 13: 0.222053\n",
      "Loss after epoch 14: 0.222048\n",
      "Loss after epoch 15: 0.222044\n",
      "Loss after epoch 16: 0.222040\n",
      "Loss after epoch 17: 0.222036\n",
      "Loss after epoch 18: 0.222032\n",
      "Loss after epoch 19: 0.222027\n",
      "Loss after epoch 20: 0.222023\n",
      "Loss after epoch 21: 0.222019\n",
      "Loss after epoch 22: 0.222015\n",
      "Loss after epoch 23: 0.222010\n",
      "Loss after epoch 24: 0.222006\n",
      "Loss after epoch 25: 0.222002\n",
      "Loss after epoch 26: 0.221997\n",
      "Loss after epoch 27: 0.221993\n",
      "Loss after epoch 28: 0.221989\n",
      "Loss after epoch 29: 0.221984\n",
      "Loss after epoch 30: 0.221980\n",
      "Loss after epoch 31: 0.221975\n",
      "Loss after epoch 32: 0.221970\n",
      "Loss after epoch 33: 0.221966\n",
      "Loss after epoch 34: 0.221961\n",
      "Loss after epoch 35: 0.221956\n",
      "Loss after epoch 36: 0.221952\n",
      "Loss after epoch 37: 0.221947\n",
      "Loss after epoch 38: 0.221942\n",
      "Loss after epoch 39: 0.221937\n",
      "Loss after epoch 40: 0.221932\n",
      "Loss after epoch 41: 0.221927\n",
      "Loss after epoch 42: 0.221922\n",
      "Loss after epoch 43: 0.221917\n",
      "Loss after epoch 44: 0.221912\n",
      "Loss after epoch 45: 0.221907\n",
      "Loss after epoch 46: 0.221902\n",
      "Loss after epoch 47: 0.221896\n",
      "Loss after epoch 48: 0.221891\n",
      "Loss after epoch 49: 0.221885\n",
      "Matriks weight hidden node:\n",
      " [matrix([[ 1.65791572,  0.39399296,  0.98660837,  2.13532702],\n",
      "        [ 1.73400141, -0.96285873,  0.86430327, -0.14296629],\n",
      "        [-0.12214924,  0.37686215,  0.11766005,  1.37995722],\n",
      "        [ 0.7553107 ,  0.12501851,  0.46022692,  0.34257517]]), matrix([[ 1.55248647, -0.34033595,  0.27766256, -1.0223786 ],\n",
      "        [-2.81426406,  0.72336555,  0.96623501, -0.81256657],\n",
      "        [ 2.62086479, -1.4951375 ,  0.13215503, -0.12276289]]), matrix([[ 2.09018854,  2.08667583,  0.22159965],\n",
      "        [ 0.45803658, -1.12678887, -2.73798488]]), matrix([[-0.89576654, -0.01584422],\n",
      "        [ 0.79366741,  1.12393128],\n",
      "        [-1.6630838 , -0.81042705]])]\n",
      "Matriks weight output node:\n",
      " [[ 0.51589293  1.04374966 -1.07831843]]\n",
      "Hasil Prediksi Train Test: \n",
      "[[0.66751547]\n",
      " [0.6662144 ]]\n"
     ]
    }
   ],
   "source": [
    "multiLayerNN = MultiLayerNN(X_train, w_hidden_node, w_output_node, var_batch, var_learning_rate, 1e-6, momentum = var_momentum, epochs = var_epoch)\n",
    "multiLayerNN.train(y_train)\n",
    "print('Matriks weight hidden node:\\n', multiLayerNN.w_hidden_node)\n",
    "print('Matriks weight output node:\\n', multiLayerNN.w_output_node)\n",
    "\n",
    "y_test_res = multiLayerNN.predict(X_test)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(y_test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier menggunakan keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2357 - acc: 0.6667\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2367 - acc: 0.6667\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2255 - acc: 0.6667\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2430 - acc: 0.6667\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 165us/step - loss: 0.2344 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2356 - acc: 0.6667\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2387 - acc: 0.6667\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2470 - acc: 0.6667\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2321 - acc: 0.6667\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2381 - acc: 0.6667\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2287 - acc: 0.6667\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2430 - acc: 0.6667\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2369 - acc: 0.6667\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2362 - acc: 0.6667\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2507 - acc: 0.6667\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2328 - acc: 0.6667\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2333 - acc: 0.6667\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2360 - acc: 0.6667\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 163us/step - loss: 0.2474 - acc: 0.6667\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2388 - acc: 0.6667\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2401 - acc: 0.6667\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2362 - acc: 0.6667\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2394 - acc: 0.6667\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.2350 - acc: 0.6667\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 164us/step - loss: 0.2222 - acc: 0.6667\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2332 - acc: 0.6667\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2385 - acc: 0.6667\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2327 - acc: 0.6667\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2360 - acc: 0.6667\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2354 - acc: 0.6667\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2251 - acc: 0.6667\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2308 - acc: 0.6667\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2351 - acc: 0.6667\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2463 - acc: 0.6667\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 249us/step - loss: 0.2371 - acc: 0.6667\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2507 - acc: 0.6667\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.2349 - acc: 0.6667\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2351 - acc: 0.6667\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2266 - acc: 0.6667\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2330 - acc: 0.6667\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2403 - acc: 0.6667\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2340 - acc: 0.6667\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2238 - acc: 0.6667\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2439 - acc: 0.6667\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2406 - acc: 0.6667\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2314 - acc: 0.6667\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 80us/step - loss: 0.2421 - acc: 0.6667\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 83us/step - loss: 0.2382 - acc: 0.6667\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2362 - acc: 0.6667\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.2309 - acc: 0.6667\n",
      "[0.25310471653938293, 0.5]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=var_batch, epochs=var_epoch, verbose=1)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Hasil Classifier A dan B\n",
    "\n",
    "Untuk batch = 1, pada algoritma 1.a loss pada awal epoch cukup besar dibanding loss awal apa 1.b, hal ini disebabkan faktor optimizer SGD yang digunakan pada model 1.b sehingga inisialisasi bobot awal dari hidden node dan output node dapat mendekati solusi optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Tugas\n",
    "1. Alvin Sullivan - 13515048 - Feed Forward and BackPropagation\n",
    "2. Albertus Djauhari - 13515054 - Eksplorasi Keras\n",
    "3. Kevin - 13515138 - Feed Forward and Backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
