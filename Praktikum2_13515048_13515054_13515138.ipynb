{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 2\n",
    "## Feed Forward Neural Network\n",
    "\n",
    "\n",
    "___Alvin Sullivan 13515048___\n",
    "\n",
    "___Albertus Djauhari Djohan 13515054___\n",
    "\n",
    "___Kevin 13515138___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Algoritma Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_nodes = [18, 17, 16]\n",
    "var_epoch = 10\n",
    "var_momentum = 0.001\n",
    "var_learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksekusi Dataset\n",
    "\n",
    "- Membaca dataset\n",
    "- Praproses Data (Continuous dan Kategorikal)\n",
    "    - Kategorikal menggunakan StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[[1 0 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "X_test\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "y_train\n",
      "[ 8 10 15 ...  7  6 10]\n",
      "y_test\n",
      "[10  8  5 ... 10  8 10]\n"
     ]
    }
   ],
   "source": [
    "file = \"dataset/data_training_praktikum.csv\"\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "#Handle Categorical Data\n",
    "le = preprocessing.LabelEncoder()\n",
    "oh = preprocessing.OneHotEncoder(sparse=False)\n",
    "data['loc'] = le.fit_transform(data['loc'])\n",
    "data['struct1'] = le.fit_transform(data['struct1'])\n",
    "data['struct2'] = le.fit_transform(data['struct2'])\n",
    "data['struct3'] = le.fit_transform(data['struct3'])\n",
    "data['cit1'] = le.fit_transform(data['cit1'])\n",
    "data['cit2'] = le.fit_transform(data['cit2'])\n",
    "data['labelclass'] = le.fit_transform(data['labelclass'])\n",
    "data = data.values\n",
    "X = data[:, 0:-1]\n",
    "y = data[:, -1]\n",
    "# y = oh.fit_transform(data[:, -1].reshape(len(data[:, -1]), 1))\n",
    "# print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=43)\n",
    "\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "\n",
    "print('X_test')\n",
    "print(X_test)\n",
    "\n",
    "print('y_train')\n",
    "print(y_train)\n",
    "\n",
    "print('y_test')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNN:\n",
    "\n",
    "    def __init__(self, data, hidden_node, output_node,\n",
    "        num_batch, learning_rate_const, tolerance_const,\n",
    "        **kwargs):\n",
    "        \n",
    "        self.instance = data\n",
    "        self.w_hidden_node = hidden_node\n",
    "        self.w_output_node = output_node\n",
    "        # Gradient Descent Parameters\n",
    "        self.batch_size = num_batch\n",
    "        self.learning_rate = learning_rate_const\n",
    "        self.tolerance = tolerance_const\n",
    "        self.momentum = kwargs.get('momentum', 0)\n",
    "        self.epochs = kwargs.get('epochs', 10)\n",
    "       \n",
    "    def train(self, instance_target):\n",
    "        instance_target_t = np.array([instance_target]).T\n",
    "        batch_iteration = int(np.ceil(self.instance.shape[0] / self.batch_size))\n",
    "        old_loss = -np.inf\n",
    "        for step in range(self.epochs):\n",
    "            loss = 0\n",
    "            for i in range(batch_iteration):\n",
    "                start_index = i * self.batch_size\n",
    "                end_index = i * self.batch_size + self.batch_size\n",
    "                if end_index > len(instance_target_t):\n",
    "                    end_index = len(instance_target_t)\n",
    "                o_out = self.gradient_descent(self.instance[start_index:end_index:1], \n",
    "                                              instance_target_t[start_index:end_index:1])\n",
    "                loss = loss + self.loss_function(o_out, \n",
    "                                            instance_target_t[start_index:end_index:1])\n",
    "        # Print Loss\n",
    "            print (\"Loss after epoch %i: %f\" % (step, loss/self.instance.shape[0]))\n",
    "            \n",
    "            if np.abs(loss - old_loss) < self.tolerance:\n",
    "                break\n",
    "            old_loss = loss\n",
    "    \n",
    "    def feed_forward(self, instance):\n",
    "        s = list()\n",
    "        o = list()\n",
    "        # Feed Forward Hidden Node\n",
    "        s_temp = instance.dot(self.w_hidden_node[0].T)\n",
    "        o_temp = self.sigmoid(s_temp)\n",
    "        s.append(s_temp)\n",
    "        o.append(o_temp)\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(1,iteration):\n",
    "            s_temp = o[i-1].dot(self.w_hidden_node[i].T)\n",
    "            o_temp = self.sigmoid(s_temp)\n",
    "            o.append(o_temp)\n",
    "        # Feed Forward Output Node\n",
    "        s_out = o[-1].dot(self.w_output_node.T)\n",
    "        o_out = self.sigmoid(s_out)\n",
    "        return s, o, s_out, o_out\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        output = 1 / (1 + np.exp(-X))\n",
    "        return np.matrix(output)\n",
    "\n",
    "    def loss_function(self, o_out, instance_target):\n",
    "        \n",
    "        squared_error = np.square(instance_target - o_out)\n",
    "        data_loss = np.sum(squared_error)      \n",
    "        return data_loss    \n",
    "    def back_propagation(self, instance_target, o, o_out):\n",
    "        d = list()\n",
    "        # Back Propagation Output Node\n",
    "        d_temp = np.multiply(np.multiply(o_out, 1-o_out), instance_target-o_out)\n",
    "        d.insert(0, d_temp)\n",
    "        # Back Propagation Hidden Node\n",
    "        d_temp = np.multiply(np.multiply(o[-1], 1-o[-1]), \n",
    "                             (self.w_output_node.T.dot(d[0].T)).T)\n",
    "        d.insert(0, d_temp)\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(iteration-1, 0, -1):\n",
    "            d_temp = np.multiply(np.multiply(o[i-1], 1-o[i-1]), \n",
    "                                 (self.w_hidden_node[i].T.dot(d[0].T)).T)\n",
    "            d.insert(0, d_temp)\n",
    "        return d\n",
    "\n",
    "    def update_weight(self, instance, o, d):\n",
    "        # Update Weight Output Node\n",
    "        self.w_output_node[0] = self.w_output_node[0] \\\n",
    "        + self.momentum * self.w_output_node[0] \\\n",
    "        + self.learning_rate * d[-1].T.dot(o[-1])\n",
    "        # Update Weight Hidden Node\n",
    "        iteration = len(self.w_hidden_node)\n",
    "        for i in range(iteration-1, 0, -1):\n",
    "            self.w_hidden_node[i] = self.w_hidden_node[i] \\\n",
    "            + self.momentum * self.w_hidden_node[i] \\\n",
    "            + self.learning_rate * d[i].T.dot(o[i-1])\n",
    "        self.w_hidden_node[0] = self.w_hidden_node[0] \\\n",
    "        + self.momentum * self.w_hidden_node[0] \\\n",
    "        + self.learning_rate * d[0].T.dot(instance)\n",
    "    def gradient_descent(self, instance, instance_target):\n",
    "        # Feed Forward\n",
    "        _,o,_,o_out = self.feed_forward(instance)\n",
    "        # Back Propagation      \n",
    "        d = self.back_propagation(instance_target, o, o_out)\n",
    "        # Update Weight\n",
    "        self.update_weight(instance, o, d)\n",
    "        return o_out\n",
    "    def predict(self, instance):\n",
    "        _,_,s_out,o_out = self.feed_forward(instance)\n",
    "        return o_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for i in range(0,len(var_nodes)):\n",
    "    if i == 0:\n",
    "        model.add(Dense(units=var_nodes[i], activation='sigmoid', \n",
    "                        input_dim=X_train.shape[1]))\n",
    "    else:\n",
    "        model.add(Dense(units=var_nodes[i], activation='sigmoid'))\n",
    "        \n",
    "sgd = optimizers.SGD(lr=var_learning_rate, decay=0, \n",
    "                     momentum=var_momentum, nesterov=False)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perbandingan Hasil Algoritma Backpropagation dan Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inisialisasi bobot awal hidden node dan output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "w_hidden_node = list()\n",
    "for i in range(0,len(var_nodes)-1):\n",
    "    if i == 0:\n",
    "        w_temp = np.random.randn(var_nodes[i],\n",
    "                                 X_train.shape[1]) / np.sqrt(var_nodes[i])\n",
    "    else:\n",
    "        w_temp = np.random.randn(var_nodes[i], \n",
    "                                 var_nodes[i-1]) / np.sqrt(var_nodes[i])\n",
    "    w_hidden_node.append(w_temp)\n",
    "\n",
    "w_output_node = np.random.randn(var_nodes[-1], var_nodes[-2])\n",
    "\n",
    "print('Matriks weight hidden node:\\n', w_hidden_node)\n",
    "print('Matriks weight output node:\\n', w_output_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Mini-Batch (Batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_batch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiLayerNN = MultiLayerNN(X_train, w_hidden_node, w_output_node, var_batch, \n",
    "                            var_learning_rate, 1e-6, \n",
    "                            momentum = var_momentum, epochs = var_epoch)\n",
    "multiLayerNN.train(y_train)\n",
    "print('Matriks weight hidden node:\\n', multiLayerNN.w_hidden_node)\n",
    "print('Matriks weight output node:\\n', multiLayerNN.w_output_node)\n",
    "\n",
    "y_test_res = multiLayerNN.predict(X_test)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(y_test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier menggunakan keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11334/11334 [==============================] - 9s 817us/step - loss: 2.1039 - acc: 0.3855\n",
      "Epoch 2/10\n",
      "11334/11334 [==============================] - 10s 874us/step - loss: 2.1045 - acc: 0.3854\n",
      "Epoch 3/10\n",
      "11334/11334 [==============================] - 11s 947us/step - loss: 2.1053 - acc: 0.3858\n",
      "Epoch 4/10\n",
      "11334/11334 [==============================] - 11s 990us/step - loss: 2.1069 - acc: 0.3857\n",
      "Epoch 5/10\n",
      "11334/11334 [==============================] - 13s 1ms/step - loss: 2.0999 - acc: 0.3858\n",
      "Epoch 6/10\n",
      "11334/11334 [==============================] - 15s 1ms/step - loss: 2.0925 - acc: 0.3855\n",
      "Epoch 7/10\n",
      "11334/11334 [==============================] - 11s 995us/step - loss: 2.0959 - acc: 0.3851\n",
      "Epoch 8/10\n",
      "11334/11334 [==============================] - 13s 1ms/step - loss: 2.0963 - acc: 0.3855\n",
      "Epoch 9/10\n",
      "11334/11334 [==============================] - 16s 1ms/step - loss: 2.0934 - acc: 0.3872\n",
      "Epoch 10/10\n",
      "11334/11334 [==============================] - 18s 2ms/step - loss: 2.0812 - acc: 0.3857\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have shape (16,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1f2f79bc65a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hasil Prediksi Train Test: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have shape (16,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, to_categorical(y_train, num_classes = 16), batch_size=var_batch, epochs=var_epoch, verbose=1)\n",
    "prediction = model.predict(X_test)\n",
    "score = model.evaluate(X_test, to_categorical(y_test, num_classes = 16), verbose=0)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(prediction)\n",
    "print(\"Hasil Pengukuran Perfomansi Train Test: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementasi Mini-Batch (Batch_size = jumlah_data)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_batch = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLayerNN = MultiLayerNN(X_train, w_hidden_node, w_output_node, var_batch, \n",
    "                            var_learning_rate, 1e-6, \n",
    "                            momentum = var_momentum, epochs = var_epoch)\n",
    "multiLayerNN.train(y_train)\n",
    "print('Matriks weight hidden node:\\n', multiLayerNN.w_hidden_node)\n",
    "print('Matriks weight output node:\\n', multiLayerNN.w_output_node)\n",
    "\n",
    "y_test_res = multiLayerNN.predict(X_test)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(y_test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier menggunakan keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11334/11334 [==============================] - 9s 762us/step - loss: 2.1488 - acc: 0.3855\n",
      "Epoch 2/10\n",
      "11334/11334 [==============================] - 9s 773us/step - loss: 2.1222 - acc: 0.3858\n",
      "Epoch 3/10\n",
      "11334/11334 [==============================] - 9s 779us/step - loss: 2.1245 - acc: 0.3857\n",
      "Epoch 4/10\n",
      "11334/11334 [==============================] - 9s 789us/step - loss: 2.1204 - acc: 0.3864\n",
      "Epoch 5/10\n",
      "11334/11334 [==============================] - 9s 800us/step - loss: 2.1379 - acc: 0.3844\n",
      "Epoch 6/10\n",
      "11334/11334 [==============================] - 10s 868us/step - loss: 2.1086 - acc: 0.3868\n",
      "Epoch 7/10\n",
      "11334/11334 [==============================] - 11s 950us/step - loss: 2.1131 - acc: 0.3873\n",
      "Epoch 8/10\n",
      "11334/11334 [==============================] - 16s 1ms/step - loss: 2.1051 - acc: 0.3871\n",
      "Epoch 9/10\n",
      "11334/11334 [==============================] - 14s 1ms/step - loss: 2.1051 - acc: 0.3862\n",
      "Epoch 10/10\n",
      "11334/11334 [==============================] - 11s 1ms/step - loss: 2.1201 - acc: 0.3857\n",
      "Hasil Prediksi Train Test: \n",
      "[[0.02801589 0.02813463 0.01971655 ... 0.09388703 0.39921883 0.4409704 ]\n",
      " [0.03706846 0.00781875 0.05605958 ... 0.17294665 0.09264085 0.2119123 ]\n",
      " [0.1843413  0.01267885 0.91170084 ... 0.65069985 0.10605599 0.415135  ]\n",
      " ...\n",
      " [0.03706846 0.00781875 0.05605958 ... 0.17294665 0.09264085 0.2119123 ]\n",
      " [0.02447044 0.02738975 0.01987069 ... 0.09622601 0.41190732 0.4624858 ]\n",
      " [0.03706846 0.00781875 0.05605958 ... 0.17294665 0.09264085 0.2119123 ]]\n",
      "Hasil Pengukuran Perfomansi Train Test: \n",
      "[2.125629686930823, 0.37698412688951644]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, to_categorical(y_train, num_classes = 16), batch_size=var_batch, epochs=var_epoch, verbose=1)\n",
    "score = model.evaluate(X_test, to_categorical(y_test, num_classes = 16), verbose=0)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"Hasil Prediksi Train Test: \")\n",
    "print(prediction)\n",
    "print(\"Hasil Pengukuran Perfomansi Train Test: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan & Analisis Hasil Classifier A dan B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Tugas\n",
    "1. Alvin Sullivan - 13515048 - \n",
    "2. Albertus Djauhari - 13515054 - \n",
    "3. Kevin - 13515138 - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
